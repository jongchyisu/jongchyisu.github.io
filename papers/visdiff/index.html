<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<!-- saved from url=(0050)http://people.cs.umass.edu/~kalo/papers/shapepfcn/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Reasoning about Fine-grained Attribute Phrases using Reference Games</title>

<link media="all" href="style.css" type="text/css" rel="stylesheet">
<style type="text/css" media="all">
img {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 10px;
	FLOAT: right;
	PADDING-BOTTOM: 10px;
	PADDING-TOP: 10px
}
#content {
	MARGIN-LEFT: auto;
 WIDTH: expression(document.body.clientWidth > 925? "925px": "auto" );
	MARGIN-RIGHT: auto;
	TEXT-ALIGN: left;
	max-width: 925px
}
body {
	TEXT-ALIGN: center
}
</style>
</head>
<body>
<div id="content">
  <h1 align="center">Reasoning about Fine-grained Attribute Phrases using Reference Games</h1>
  <img alt="teaser" src="teaser.png" width="500">
  <br>
  <br>
  <br>
  <h2>People</h2>
  <ul id="people">
    <li><a href="http://people.cs.umass.edu/~jcsu/">Jong-Chyi Su*</a> </li>
    <li><a href="http://people.cs.umass.edu/~chenyun/">Chenyun Wu*</a> </li>
    <li><a href="http://people.cs.umass.edu/~smaji/">Subhransu Maji</a> </li>
    <li><a href="http://people.cs.umass.edu/~hzjiang/">Huaizu Jiang</a> </li>
  </ul>
  <br>
  <br>
  <h2>Abstract</h2>
  <p align="justify">We present a framework for learning to describe fine-grained visual differences between instances using attribute phrases. <b>Attribute phrases</b> capture distinguishing aspects of an object (e.g., <i>"propeller on the nose"</i> or <i>"door near the wing"</i> for airplanes) in a compositional manner. Instances within a category can be described by a set of these phrases and collectively they span the space of semantic attributes for a category. We collect a large dataset of such phrases by asking annotators to describe several visual differences between a pair of instances within a category. We then learn to describe and ground these phrases to images in the context of a <b>reference game</b> between a speaker and a listener. The goal of a speaker is to describe attributes of an image that allows the listener to correctly identify it within a pair. Data collected in a pairwise manner improves the ability of the speaker to generate, and the ability of the listener to interpret visual descriptions. Moreover, due to the compositionality of attribute phrases, the trained listeners can interpret descriptions not seen during training for image retrieval, and the speakers can generate attribute-based explanations for differences between previously unseen categories. We also show that embedding an image into the semantic space of attribute phrases derived from listeners offers <b>20%</b> improvement in accuracy over existing attribute-based representations on the FGVC-aircraft dataset. </p>
  <br/>

  <h2>Publication</h2> 
  <b>Reasoning about Fine-grained Attribute Phrases using Reference Games</b>,
  <br/>
  Jong-Chyi Su*, Chenyun Wu*, Huaizu Jiang, and Subhransu Maji
  <br/>
  <em>International Conference on Computer Vision (ICCV), 2017</em>
  <br/>
  <a href="http://people.cs.umass.edu/~jcsu/papers/visdiff/visdiff.pdf">pdf</a>,
  <a href="https://arxiv.org/abs/1708.08874">arXiv</a>,
  <a href="http://supermoe.cs.umass.edu/visdiff/visdiff-supp.pdf">supplementary material (4.1MB)</a>,
  <a href="http://people.cs.umass.edu/~jcsu/papers/visdiff/visdiff.bib">bibtex</a>,
  <a href="http://supermoe.cs.umass.edu/visdiff/poster.pdf">poster (16MB)</a>,
  <a href="http://supermoe.cs.umass.edu/visdiff/slides.pdf">slides (12MB)</a>
  <br/>
  <br/>

  <h2>Dataset and Source Code</h2>
  <p><a href="https://github.com/jongchyisu/attribute_phrases">Github page</a></p>
  <br/>

  <h2>More Figures of Our Results in High Resolution</h2>
  <a href="http://supermoe.cs.umass.edu/visdiff/dataset.pdf">More examples from our dataset</a><br/>
  <a href="http://supermoe.cs.umass.edu/visdiff/pds.pdf">More results of generated attribute phrases</a><br/>
  <a href="http://supermoe.cs.umass.edu/visdiff/set_wise.pdf">Set-wise attribute phrases (11MB)</a><br/>
  <a href="http://supermoe.cs.umass.edu/visdiff/L_tsne.pdf">t-SNE embedding of <i>attribute phrases</i> with simple listener model</a><br/>
  <a href="http://supermoe.cs.umass.edu/visdiff/DL_tsne.pdf">t-SNE embedding of <i>attribute phrases</i> with discerning listener model</a><br/>
  <a href="http://supermoe.cs.umass.edu/visdiff/image_L_tsne.pdf">t-SNE embedding of <i>images</i> with simple listener model</a><br/>
  <br/>

  <h2>Image Retrieval Demo</h2>
  <a href="http://davinci.cs.umass.edu/demo.php">Click here</a> for image retrieval demo with attribute phrases using the listener model
  <br/>
  <br/>

  <h2>Acknowledgements</h2>
  This research was supported in part by the NSF grants 1617917 and 1661259, and a faculty gift from Facebook. The experiments were performed using equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Tech Collaborative and GPUs donated by NVIDIA.
  <br>
  <div id="footer"><a href="http://people.cs.umass.edu/~jcsu/">back to Jong-Chyi's page</a></div>
</div>


</body></html>
